{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Part I. ETL Pipeline for Pre-Processing the Files",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### Import Python packages ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Import necessary Python packages\nimport pandas as pd\nimport cassandra\nimport os\nimport glob\nimport csv",
      "metadata": {},
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Creating list of filepaths to process original event csv data files",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Creating list of filepaths to process original event csv data files\n# Get your current folder and subfolder event data\nfilepath = os.getcwd() + '/event_data'\n\n# Create a list of files and collect each filepath\nfile_path_list = []\nfor root, dirs, files in os.walk(filepath):\n    # Join the file path and roots with the subdirectories using glob\n    file_path_list = glob.glob(os.path.join(root, '*'))",
      "metadata": {},
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Processing the files to create the data file csv that will be used for Apache Cassandra tables\nfull_data_rows_list = []\nfor f in file_path_list:\n    with open(f, 'r', encoding='utf8', newline='') as csvfile:\n        csvreader = csv.reader(csvfile)\n        next(csvreader)\n        for line in csvreader:\n            full_data_rows_list.append(line)\n\n# Create smaller event data csv file used to insert data into the Apache Cassandra tables\ncsv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\noutput_file = 'event_datafile_new.csv'\nwith open(output_file, 'w', encoding='utf8', newline='') as f:\n    writer = csv.writer(f, dialect='myDialect')\n    writer.writerow(['artist', 'firstName', 'gender', 'itemInSession', 'lastName', 'length', 'level', 'location', 'sessionId', 'song', 'userId'])\n    for row in full_data_rows_list:\n        if row[0] == '':\n            continue\n        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n",
      "metadata": {},
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Verify the number of rows in the generated csv file\nwith open(output_file, 'r', encoding='utf8') as f:\n    print(f\"Number of rows in the CSV file: {sum(1 for line in f)}\")",
      "metadata": {},
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "12\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Part II: Complete the Apache Cassandra coding portion of your project.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### Creating a Cluster",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Creating a Cluster\n# Connect to a Cassandra instance on your local machine (127.0.0.1)\nfrom cassandra.cluster import Cluster\ncluster = Cluster()\n\n# Establish connection and begin executing queries, initialize a session\nsession = cluster.connect()",
      "metadata": {},
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Create Keyspace",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create Keyspace\nsession.execute(\"\"\"\n    CREATE KEYSPACE IF NOT EXISTS udacity \n    WITH REPLICATION = \n    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\n\"\"\")\nsession.set_keyspace('udacity')",
      "metadata": {},
      "execution_count": 6,
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cassandra.cluster.ResultSet at 0x7f77fa57d048>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Create queries to ask the following three questions of the data\n\n### Question 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n\n## Answer 1:\n\n## To obtain the artist's name, song title, and song length from the music app history, we need to focus on sessionId and itemInSession as filtering criteria. Given the nature of NoSQL databases, we should first design our query and then create the necessary table.\n\n## The desired output is the \"Name of the artist, title of the song, and length of the track\" based on \"sessionId and itemInSession.\" This indicates that our SELECT statement should look like this:\n\n## SELECT artist_name, song_title, song_length\n## FROM song_session\n## WHERE sessionId = 338 AND itemInSession = 4\n\n## Now, let's proceed with creating the table. We'll include a check to ensure the table only gets created if it doesn't already exist. We'll name the table 'song_session' as it aptly describes its purpose.\n\n## Column Names:\n## We require the artist's name, song title, song length, sessionId, and itemInSession for our query. Therefore, the column names will be artist, song, length, sessionId, and itemInSession.\n\n## Primary Key:\n## To uniquely identify each row, we'll use sessionId and itemInSession as the primary key since these two fields align with the query's filtering criteria.\n\n## Please refer to the Query 1 code below. \n## The code will create the 'song_session' table, insert data from the CSV file, execute the query based on sessionId and itemInSession, and display the artist's name, song title, and song length for the specified session and item.\n\n\n### Question 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n## Answer 2: \n## The Question 2 requires us to retrieve the name of the artist, song title (sorted by itemInSession), and the user's first and last names based on a specific userId and sessionId.\n\n## The expected output is: \"Name of the artist, song title sorted by itemInSession, and user's first and last name\" Based on: \"userId and sessionId\"\n\n## From the above two points, we know that the query to get the data will be a SELECT statement like:\n\n## SELECT name_of_artist, song_title, first_name, last_name FROM user_session WHERE userId = 10 AND sessionId = 182 ORDER BY itemInSession ASC;\n\n## To facilitate this query, we need to create a table with appropriate columns and primary keys. The primary key should include userId and sessionId to allow filtering on these columns, and include itemInSession as a clustering column to sort the data by this column.\n\n## We can create the table with a statement like:\n\n\n## CREATE TABLE IF NOT EXISTS user_session (\n    userId INT, \n    sessionId INT, \n    itemInSession INT, \n    name_of_artist TEXT, \n    song_title TEXT, \n    first_name TEXT, \n    last_name TEXT, \n    PRIMARY KEY ((userId, sessionId), itemInSession)\n);\n## Column Names: The columns in the table will be userId, sessionId, itemInSession, name_of_artist, song_title, first_name, and last_name.\n\n## Primary Key: The composite partition key will be userId and sessionId to ensure the data is distributed evenly and allow filtering on these columns, and itemInSession will be a clustering column to enable sorting by this column.\n\n\n\n### Question 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n## Answer 3: \n## The Question 3 requires us to retrieve the first and last names of every user who listened to a specific song titled 'All Hands Against His Own'.\n\n## The expected output is: \"User's first and last name\"\n## Based on: \"Song title\"\n\n## From the above two points, we know that the query to get the data will be a SELECT statement like:\n\n\n## SELECT first_name, last_name FROM song_history WHERE song_title = 'All Hands Against His Own';\n## To facilitate this query, we need to create a table with appropriate columns and primary keys. The primary key should include the song title to allow filtering on this column, and also include userId to uniquely identify each record.\n\n## We can create the table with a statement like:\n\n## CREATE TABLE IF NOT EXISTS song_history (\n    song_title TEXT, \n    userId INT, \n    first_name TEXT, \n    last_name TEXT, \n    PRIMARY KEY (song_title, userId)\n);\n## Column Names: The columns in the table will be song_title, userId, first_name, and last_name.\n\n## Primary Key: The primary key will be a composite of song_title and userId to enable filtering on the song title and ensure that each record is unique.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Creating queries to analyze the data\n# Create table (artist_song_for_session) for query 1\n# The columns are ordered by the parition key, clustering column then all of the returned values\nquery = \"CREATE TABLE IF NOT EXISTS artist_song_for_session \"\nquery += \"(sessionId int, itemInSession int, artist text, song text, length float, PRIMARY KEY (sessionId, itemInSession))\"\nsession.execute(query)\n",
      "metadata": {},
      "execution_count": 8,
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cassandra.cluster.ResultSet at 0x7f77fa578240>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# Insert data into table for query 1\nfile = 'event_datafile_new.csv'\nwith open(file, encoding = 'utf8') as f:\n    csvreader = csv.reader(f)\n    next(csvreader)\n    for line in csvreader:\n        query = \"INSERT INTO artist_song_for_session (sessionId, itemInSession, artist, song, length)\"\n        query += \"VALUES (%s, %s, %s, %s, %s)\"\n        session.execute(query, (int(line[8]), int(line[3]), line[0], line[9], float(line[5])))\n",
      "metadata": {},
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Do a SELECT to verify that the data have been inserted into each table",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Perform query\nquery = \"SELECT artist, song, length from artist_song_for_session WHERE sessionId=338 AND itemInSession=4\"\nrows = session.execute(query)\nfor row in rows:\n    print(row.artist, row.song, row.length)\n",
      "metadata": {
        "scrolled": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\n# Create table (artist_song_user_for_user_session) for query 2\n# The columns are ordered by the parition key, clustering columns then all of the returned values\nquery = \"CREATE TABLE IF NOT EXISTS artist_song_user_for_user_session \"\nquery += \"(userId int, sessionId int, itemInSession int, artist text, song text, firstName text, lastName text, PRIMARY KEY ((userId, sessionId), itemInSession))\"\nsession.execute(query)\n\n\n# Insert data into table for query 2\nfile = 'event_datafile_new.csv'\nwith open(file, encoding = 'utf8') as f:\n    csvreader = csv.reader(f)\n    next(csvreader)\n    for line in csvreader:\n        query = \"INSERT INTO artist_song_user_for_user_session (userId, sessionId, itemInSession, artist, song, firstName, lastName)\"\n        query += \"VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n        session.execute(query, (int(line[10]), int(line[8]), int(line[3]), line[0], line[9], line[1], line[4]))\n\n\n                    ",
      "metadata": {},
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Perform query 2\nquery = \"SELECT artist, song, firstName, lastName FROM artist_song_user_for_user_session WHERE userId=10 AND sessionId=182\"\nrows = session.execute(query)\nfor row in rows:\n    print(row.artist, row.song, row.firstName, row.lastName)\n               ",
      "metadata": {},
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Create table (user_for_song) for query 3\n# The columns are ordered by the parition key, clustering column then all of the returned values\n# The main change in query 2 was ensuring the column order in the INSERT statement matches the order defined in the CREATE statement \n\nquery = \"CREATE TABLE IF NOT EXISTS user_for_song \"\nquery += \"(song text, userId int, firstName text, lastName text, PRIMARY KEY (song, userId))\"\nsession.execute(query)\n\n\n# Insert data into table for query 3\nfile = 'event_datafile_new.csv'\nwith open(file, encoding = 'utf8') as f:\n    csvreader = csv.reader(f)\n    next(csvreader)\n    for line in csvreader:\n        query = \"INSERT INTO user_for_song (song, userId, firstName, lastName)\"\n        query += \"VALUES (%s, %s, %s, %s)\"\n        session.execute(query, (line[9], int(line[10]), line[1], line[4]))\n",
      "metadata": {},
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Perform query 3\nquery = \"SELECT firstName, lastName FROM user_for_song WHERE song='All Hands Against His Own'\"\nrows = session.execute(query)\nfor row in rows:\n    print(row.firstName, row.lastName)\n",
      "metadata": {},
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Drop the tables before closing out the sessions",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Cleaning up resources - dropping tables before closing the sessions\nsession.execute(\"DROP TABLE artist_song_for_session\")\nsession.execute(\"DROP TABLE artist_song_user_for_user_session\")\nsession.execute(\"DROP TABLE user_for_song\")\n",
      "metadata": {},
      "execution_count": 15,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cassandra.cluster.ResultSet at 0x7f77fa572278>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Close the session and cluster connection¶",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Confirming the closure of session and cluster connection\nsession.shutdown()\ncluster.shutdown()\nprint(\"Session and cluster connections have been closed.\")",
      "metadata": {},
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}